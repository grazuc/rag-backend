"""
ingest.py
----------
Ingiere documentos en m√∫ltiples formatos desde ./docs, los trocea,
genera embeddings y los guarda en la base de datos vectorial.
Soporta procesamiento paralelo, procesamiento incremental y chunking din√°mico.
"""

import os
import time
import hashlib
import argparse
import logging
import json
import re
import sys
from typing import List, Dict, Optional, Set, Tuple, Any
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from langchain_community.embeddings import HuggingFaceEmbeddings
import multiprocessing
import shutil
import pickle
from datetime import datetime
import statistics

import numpy as np
import torch
from dotenv import load_dotenv
from tqdm import tqdm
from langdetect import detect, LangDetectException

from langchain_community.document_loaders import (
    PyPDFLoader, 
    UnstructuredFileLoader,
    DirectoryLoader,
    TextLoader, 
    Docx2txtLoader
)
from langchain_community.document_loaders.pdf import PyPDFium2Loader
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter
)
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores.pgvector import PGVector
from threading import Lock
import psutil
import traceback

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("ingest.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Configuraci√≥n por defecto
@dataclass
class IngestConfig:
    docs_dir: Path = Path("docs")
    cache_dir: Path = Path(".cache")
    embed_model: str = "intfloat/multilingual-e5-base" 
    collection_name: str = "manual_e5_multi"
    chunk_size: int = 384  # Chunk size base
    chunk_overlap: int = 64  # Chunk overlap base
    batch_size: int = 128  # Batch size m√°s grande para mejor rendimiento
    max_workers: int = None  # Permitir autodetecci√≥n
    max_embed_workers: int = 1  # Workers para embeddings
    allowed_languages: Set[str] = field(default_factory=lambda: {"es", "en"})
    file_extensions: List[str] = field(default_factory=lambda: ["pdf", "txt", "docx", "md"])
    deduplication: bool = True
    similarity_threshold: float = 0.95  # Para deduplicaci√≥n
    incremental: bool = True  # Procesamiento incremental de archivos
    semantic_chunking: bool = True  # Chunking sem√°ntico
    dynamic_chunking: bool = True  # Chunking din√°mico adaptativo
    reset_vector_collection: bool = True # Nuevo campo


# 1) Entorno
def load_config() -> Tuple[IngestConfig, str]:
    """Carga la configuraci√≥n desde argumentos y env vars"""
    load_dotenv()
    
    # Verificar variable de entorno obligatoria
    pg_conn = os.environ.get("PG_CONN")
    if not pg_conn:
        raise EnvironmentError("Variable de entorno PG_CONN no definida")
    
    # Parsear argumentos de l√≠nea de comandos
    parser = argparse.ArgumentParser(description="Ingesta de documentos para RAG")
    parser.add_argument("--docs-dir", type=str, default="docs", 
                        help="Directorio con documentos a ingerir")
    parser.add_argument("--cache-dir", type=str, default=".cache",
                        help="Directorio para cach√© de procesamiento")
    parser.add_argument("--model", type=str, default="intfloat/multilingual-e5-base",
                        help="Modelo de embeddings a utilizar")
    parser.add_argument("--collection", type=str, default="manual_e5_multi",
                        help="Nombre de la colecci√≥n en PGVector")
    parser.add_argument("--chunk-size", type=int, default=384,
                        help="Tama√±o base de los chunks de texto")
    parser.add_argument("--chunk-overlap", type=int, default=64,
                        help="Superposici√≥n base entre chunks")
    parser.add_argument("--batch-size", type=int, default=128,
                        help="Tama√±o de lote para inserci√≥n en BD")
    parser.add_argument("--workers", type=int, default=None,
                        help="N√∫mero de workers para procesamiento paralelo (None=auto)")
    parser.add_argument("--embed-workers", type=int, default=1,
                        help="N√∫mero de workers para embeddings paralelos")
    parser.add_argument("--langs", type=str, default="es,en",
                        help="Idiomas permitidos (c√≥digos ISO separados por comas)")
    parser.add_argument("--extensions", type=str, default="pdf,txt,docx,md",
                        help="Extensiones de archivo a procesar (separadas por comas)")
    parser.add_argument("--no-dedup", action="store_true",
                        help="Desactivar deduplicaci√≥n de chunks")
    parser.add_argument("--no-incremental", action="store_true",
                        help="Desactivar procesamiento incremental")
    parser.add_argument("--no-semantic", action="store_true",
                        help="Desactivar chunking sem√°ntico")
    parser.add_argument("--no-dynamic", action="store_true",
                        help="Desactivar chunking din√°mico adaptativo")
    parser.add_argument("--reset-cache", action="store_true",
                        help="Eliminar cach√© y reprocesar todos los archivos")
    parser.add_argument("--reset-vector-collection", action="store_true",
    help="Eliminar y recrear la colecci√≥n en PGVector antes de la ingesta.")
    
    args = parser.parse_args()
    
    # Auto-detecci√≥n del n√∫mero de workers si no se especifica
    max_workers = args.workers if args.workers is not None else max(1, multiprocessing.cpu_count() - 1)
    
    # Crear configuraci√≥n
    config = IngestConfig(
        docs_dir=Path(args.docs_dir),
        cache_dir=Path(args.cache_dir),
        embed_model=args.model,
        collection_name=args.collection,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        batch_size=args.batch_size,
        max_workers=max_workers,
        max_embed_workers=args.embed_workers,
        allowed_languages=set(args.langs.split(",")),
        file_extensions=args.extensions.split(","),
        deduplication=not args.no_dedup,
        incremental=not args.no_incremental,
        semantic_chunking=not args.no_semantic,
        dynamic_chunking=not args.no_dynamic,
        reset_vector_collection=args.reset_vector_collection
    )
    
    # Resetear cach√© si se solicita
    if args.reset_cache and config.cache_dir.exists():
        shutil.rmtree(config.cache_dir)
    
    # Asegurar que existe el directorio de cach√©
    config.cache_dir.mkdir(exist_ok=True, parents=True)
    
    return config, pg_conn


class DocumentProcessor:
    """Clase para manejo de procesamiento de documentos con cach√©"""
    
    def __init__(self, config: IngestConfig):
        self.config = config
        self.cache_file = config.cache_dir / "processed_files.json"
        self.processed_files = self._load_processed_files()
        
    def _load_processed_files(self) -> Dict[str, Dict[str, Any]]:
        """Carga el registro de archivos procesados desde cach√©"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Error al cargar cach√©: {e}")
                return {}
        return {}
    
    def _save_processed_files(self):
        """Guarda el registro de archivos procesados en cach√©"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.processed_files, f)
    
    def get_files_to_process(self) -> List[Tuple[Path, bool]]:
        """
        Devuelve lista de archivos a procesar, indicando si necesitan procesamiento
        Retorna: Lista de tuplas (archivo, necesita_procesamiento)
        """
        files_to_process = []
        
        for ext in self.config.file_extensions:
            for file_path in self.config.docs_dir.rglob(f"*.{ext}"):
                # Obtener informaci√≥n del archivo
                file_key = str(file_path.relative_to(self.config.docs_dir))
                file_stat = file_path.stat()
                file_size = file_stat.st_size
                file_mtime = file_stat.st_mtime
                
                # Verificar si el archivo necesita procesamiento
                needs_processing = True
                if self.config.incremental and file_key in self.processed_files:
                    cached_info = self.processed_files[file_key]
                    # Si tama√±o y fecha de modificaci√≥n coinciden, no procesar
                    if cached_info.get("size") == file_size and cached_info.get("mtime") == file_mtime:
                        needs_processing = False
                
                files_to_process.append((file_path, needs_processing))
        
        return files_to_process
    
    def mark_file_processed(self, file_path: Path):
        """Marca un archivo como procesado en el cach√©"""
        file_stat = file_path.stat()
        self.processed_files[file_path.name] = {
            "size": file_stat.st_size,
            "mtime": file_stat.st_mtime,
            "last_processed": time.time()
        }
        self._save_processed_files()
    
    def get_chunks_cache_path(self, file_path: Path) -> Path:
        """Devuelve la ruta para cach√© de chunks de un archivo"""
        file_hash = hashlib.md5(str(file_path).encode()).hexdigest()
        return self.config.cache_dir / f"chunks_{file_hash}.pkl"
    
    def get_cached_chunks(self, file_path: Path) -> Optional[List[Document]]:
        """Intenta obtener chunks cacheados para un archivo"""
        if not self.config.incremental:
            return None
            
        cache_path = self.get_chunks_cache_path(file_path)
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"Error al cargar chunks desde cach√©: {e}")
        return None
    
    def save_chunks_to_cache(self, file_path: Path, chunks: List[Document]):
        """Guarda chunks en cach√© para un archivo"""
        if not self.config.incremental:
            return
            
        cache_path = self.get_chunks_cache_path(file_path)
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(chunks, f)
        except Exception as e:
            logger.warning(f"Error al guardar chunks en cach√©: {e}")


def get_file_loader(file_path: Path, allowed_extensions):
    """Selecciona el loader m√°s adecuado seg√∫n el tipo de archivo con mejor manejo de errores"""
    extension = file_path.suffix.lower().lstrip(".")
    
    if extension not in allowed_extensions:
        raise ValueError(f"Extensi√≥n no soportada: {extension}")
    
    try:
        # Verificar que el archivo existe y tiene tama√±o
        if not file_path.exists():
            raise FileNotFoundError(f"El archivo {file_path} no existe")
        
        if file_path.stat().st_size == 0:
            raise ValueError(f"El archivo {file_path} est√° vac√≠o")
        
        if extension == "pdf":
            # Intentar primero con PyPDFium2Loader, y si falla, usar PyPDFLoader como respaldo
            try:
                return PyPDFium2Loader(str(file_path))
            except Exception as e:
                logger.warning(f"Error con PyPDFium2Loader: {e}, intentando con PyPDFLoader")
                return PyPDFLoader(str(file_path))
        elif extension == "docx":
            return Docx2txtLoader(str(file_path))
        elif extension == "md":
            return TextLoader(str(file_path), encoding='utf-8', autodetect_encoding=True)
        elif extension == "txt":
            return TextLoader(str(file_path), encoding='utf-8', autodetect_encoding=True)
        else:
            return UnstructuredFileLoader(str(file_path))
    except Exception as e:
        logger.error(f"Error al crear loader para {file_path}: {e}")
        raise


# En ingest.py

def detect_language(text: str, min_length_for_detection: int = 20, fallback_lang="es") -> Optional[str]:
    """
    Detecta el idioma del texto con mejor manejo de textos cortos y casos l√≠mite.
    Incluye fallback para manejar casos donde la detecci√≥n no es confiable.
    """
    stripped_text = text.strip()
    # Usar un umbral m√°s bajo para mejorar detecci√≥n en chunks peque√±os
    if not stripped_text:
        return fallback_lang  # Retornar idioma por defecto si est√° vac√≠o
        
    if len(stripped_text) < min_length_for_detection:
        # Para textos muy cortos, intentar detectar pero con precauci√≥n
        logger.debug(f"Texto corto para detecci√≥n confiable (longitud: {len(stripped_text)})")
        # Si contiene palabras clave en espa√±ol, podemos asumir espa√±ol
        if re.search(r'\b(el|la|los|las|de|en|por|para|con|que)\b', stripped_text.lower()):
            return "es"
        # Si contiene palabras clave en ingl√©s
        elif re.search(r'\b(the|of|in|to|and|that|for|with)\b', stripped_text.lower()):
            return "en"
        return fallback_lang
    
    try:
        # Usar langdetect con mayor robustez
        lang = detect(stripped_text)
        return lang
    except LangDetectException as e:
        logger.warning(f"Fallo en detecci√≥n de idioma: '{stripped_text[:50]}...'. Error: {e}")
        return fallback_lang


def sanitize_pg_identifier(name: str) -> str:
    """Convierte un nombre arbitrario a un identificador SQL seguro para PostgreSQL"""
    return re.sub(r'[^a-zA-Z0-9_]', '_', name)

def compute_text_hash(text: str) -> str:
    """Calcula un hash del texto para deduplicaci√≥n"""
    return hashlib.md5(text.encode("utf-8")).hexdigest()


def analyze_document_structure(text: str, file_extension: str) -> Dict[str, Any]:
    """
    Analiza la estructura del documento para determinar caracter√≠sticas relevantes
    para el chunking din√°mico, con mejor identificaci√≥n de tipos de documento
    y an√°lisis m√°s profundo de la estructura.
    """
    # Inicializar an√°lisis con valores por defecto mejorados
    analysis = {
        "extension": file_extension,
        "doc_type": "general",
        "avg_sentence_length": 0,
        "avg_paragraph_length": 0,
        "semantic_density": 0.0,
        "has_structures": False,
        "has_tables": False,
        "has_code": False,
        "has_lists": False,
        "title": None,
        "language": None,
        "recommended_chunk_size": 450,  # Valor por defecto mejorado
        "recommended_chunk_overlap": 80  # Valor por defecto mejorado
    }
    
    # Intento de detecci√≥n de idioma para ajustar an√°lisis
    try:
        analysis["language"] = detect_language(text[:3000])  # Usar una muestra m√°s grande para mejor detecci√≥n
    except:
        pass  # Si falla, seguimos sin idioma detectado
    
    # Detecci√≥n m√°s sofisticada del tipo de documento
    if file_extension == "pdf":
        analysis["doc_type"] = "pdf"
        # Detectar si es un PDF t√©cnico, acad√©mico o general
        if re.search(r"(?i)(abstract|keywords|references|bibliography|doi|isbn)", text[:3000]):
            analysis["doc_type"] = "scientific_pdf"
        elif re.search(r"(?i)(contrato|acuerdo|cl√°usula|art√≠culo|ley|decreto|resoluci√≥n|expediente)", text[:3000]):
            analysis["doc_type"] = "legal_pdf"
    elif file_extension == "docx":
        analysis["doc_type"] = "docx"
    elif file_extension == "md":
        analysis["doc_type"] = "markdown"
        # Detectar si es documentaci√≥n t√©cnica
        if re.search(r"```(python|javascript|java|c\+\+|bash|sql)", text):
            analysis["doc_type"] = "technical_markdown"
            analysis["has_code"] = True
    elif file_extension == "txt":
        # An√°lisis m√°s profundo del tipo de texto
        if re.search(r"(?i)(art√≠culo|cl√°usula|contrato|acuerdo|legal|legislaci√≥n)", text[:2000]):
            analysis["doc_type"] = "legal"
        elif re.search(r"(?i)(from:|to:|subject:|sent:|cc:|bcc:)", text[:1000]):
            analysis["doc_type"] = "email"
        elif re.search(r"(?i)(abstract|introducci√≥n|metodolog√≠a|conclusi√≥n|references)", text[:3000]):
            analysis["doc_type"] = "scientific"
        elif re.search(r"(?i)(def |class |import |function |var |const |let |```python|```javascript)", text):
            analysis["doc_type"] = "code"
            analysis["has_code"] = True
    
    # Extraer t√≠tulo si es posible
    title_match = re.search(r"(?i)^(?:\s*#\s*|\s*t√≠tulo\s*:?\s*|\s*title\s*:?\s*)(.*?)$", text[:1000], re.MULTILINE)
    if title_match:
        analysis["title"] = title_match.group(1).strip()
    
    # An√°lisis de estructura mejorado
    paragraphs = re.split(r'\n\s*\n', text)
    if len(paragraphs) > 1:
        paragraph_lengths = [len(p.split()) for p in paragraphs if p.strip()]
        if paragraph_lengths:
            analysis["avg_paragraph_length"] = sum(paragraph_lengths) / len(paragraph_lengths)
            analysis["paragraph_count"] = len(paragraph_lengths)
            # Analizar variabilidad en longitud de p√°rrafos
            if len(paragraph_lengths) > 3:
                analysis["paragraph_length_std"] = statistics.stdev(paragraph_lengths)
            else:
                analysis["paragraph_length_std"] = 0
    
    # An√°lisis de oraciones m√°s preciso
    sentence_pattern = r'[.!?][\s"\')\]]+'  # Mejor regex para detectar finales de oraci√≥n
    sentences = re.split(sentence_pattern, text)
    if sentences:
        sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
        if sentence_lengths:
            analysis["avg_sentence_length"] = sum(sentence_lengths) / len(sentence_lengths)
            analysis["sentence_count"] = len(sentence_lengths)
            
            # Medir complejidad por desviaci√≥n est√°ndar de longitud de oraciones
            if len(sentence_lengths) > 5:
                analysis["sentence_length_std"] = statistics.stdev(sentence_lengths)
                # Complejidad: alta desviaci√≥n est√°ndar indica texto con estructuras variadas
                analysis["complexity"] = min(1.0, analysis["sentence_length_std"] / 10)
            else:
                analysis["sentence_length_std"] = 0
                analysis["complexity"] = 0.2  # Valor predeterminado bajo para textos cortos
    
    # An√°lisis de densidad sem√°ntica mejorado (aproximaci√≥n por unique words / total words)
    words = re.findall(r'\b\w+\b', text.lower())
    if words:
        analysis["word_count"] = len(words)
        unique_words = set(words)
        analysis["unique_word_count"] = len(unique_words)
        # Calcular densidad sem√°ntica con penalizaci√≥n para textos cortos
        min_words_for_full_density = 100  # Umbral para considerar muestra suficiente
        discount_factor = min(1.0, len(words) / min_words_for_full_density)
        analysis["semantic_density"] = (len(unique_words) / max(1, len(words))) * discount_factor
    
    # Detectar estructuras especiales
    # Tablas
    if re.search(r'[|+][-+]+[|+]', text) or re.search(r'\b\d+\s*&\s*\d+\b', text):
        analysis["has_tables"] = True
        analysis["has_structures"] = True
    
    # Listas
    if re.search(r'^\s*[*\-+‚Ä¢‚óã‚ó¶]\s+\w+.*$', text, re.MULTILINE) or \
       re.search(r'^\s*\d+\.\s+\w+.*$', text, re.MULTILINE):
        analysis["has_lists"] = True
        analysis["has_structures"] = True
    
    # Bloques de c√≥digo
    if re.search(r'```.*?```', text, re.DOTALL) or re.search(r'<code>.*?</code>', text, re.DOTALL):
        analysis["has_code"] = True
        analysis["has_structures"] = True
    
    # URLs y referencias
    urls_count = len(re.findall(r'https?://\S+', text))
    if urls_count > 0:
        analysis["urls_count"] = urls_count
        analysis["has_references"] = True
    
    # Determinar chunk_size y chunk_overlap basados en el an√°lisis
    determine_chunking_parameters(analysis)
    
    return analysis


def determine_chunking_parameters(analysis: Dict[str, Any]):
    """
    Determina los par√°metros √≥ptimos de chunking basados en el an√°lisis del documento,
    con mejor adaptaci√≥n seg√∫n el tipo de contenido y posibilidad de recuperaci√≥n contextual.
    """
    doc_type = analysis["doc_type"]
    
    # Chunking m√°s inteligente por tipo de documento
    if doc_type == "legal":
        # Documentos legales requieren chunks m√°s grandes y mayor overlap para mantener contexto legal
        base_chunk_size = 600
        base_chunk_overlap = 150  # Mayor overlap para mejor contexto de referencias legales
    elif doc_type == "scientific":
        # Documentos cient√≠ficos con chunks medianos pero mayor overlap
        base_chunk_size = 500
        base_chunk_overlap = 120  # Mayor overlap para mantener coherencia en ecuaciones y referencias
    elif doc_type == "email":
        # Emails t√≠picamente son m√°s cortos y autocontenidos
        base_chunk_size = 350
        base_chunk_overlap = 60
    elif doc_type == "markdown":
        # Markdown ya tiene estructura que ayuda a la segmentaci√≥n
        base_chunk_size = 450
        base_chunk_overlap = 90
    else:
        # Valores base para otros tipos, ligeramente m√°s grandes que los anteriores
        base_chunk_size = 450
        base_chunk_overlap = 80
    
    # Ajustes por densidad sem√°ntica con mayor sensibilidad
    density_factor = 1.0
    if analysis["semantic_density"] > 0.75:  # Alta diversidad l√©xica = texto t√©cnico/especializado
        # Chunks m√°s peque√±os para texto denso, pero mayor overlap
        density_factor = 0.85
        # Aumentar overlap proporcionalmente
        base_chunk_overlap = int(base_chunk_overlap * 1.2)
    elif analysis["semantic_density"] < 0.35:  # Baja diversidad l√©xica = texto simple/repetitivo
        # Chunks m√°s grandes para texto simple
        density_factor = 1.3
    
    # Ajustes por longitud de oraciones con mejor calibraci√≥n
    sentence_factor = 1.0
    avg_sentence_length = analysis.get("avg_sentence_length", 0)
    if avg_sentence_length > 30:  # Oraciones muy largas (ej. texto acad√©mico o legal)
        # Reducir tama√±o para evitar cortar en medio de ideas complejas
        sentence_factor = 0.75
        # Mayor overlap para texto con oraciones complejas
        base_chunk_overlap = int(base_chunk_overlap * 1.3)
    elif avg_sentence_length > 20:  # Oraciones largas
        sentence_factor = 0.85
        base_chunk_overlap = int(base_chunk_overlap * 1.15)
    elif avg_sentence_length < 8:  # Oraciones muy cortas (di√°logos, listas, etc.)
        # Aumentar tama√±o para capturar suficiente contexto
        sentence_factor = 1.2
    
    # Ajuste para documentos estructurados
    structure_factor = 0.85 if analysis["has_structures"] else 1.0
    
    # Ajuste por complejidad 
    complexity = analysis.get("complexity", 0)
    # Mayor complejidad = m√°s variabilidad en las oraciones = necesita chunks m√°s adaptativos
    complexity_factor = max(0.8, min(1.2, 1.0 - (complexity * 0.2)))
    
    # Calcular tama√±os finales ajustados con todos los factores
    chunk_size = int(base_chunk_size * density_factor * sentence_factor * structure_factor * complexity_factor)
    
    # Ajustar overlap basado en todos los factores, pero especialmente en complejidad
    # Cuando el texto es m√°s complejo, necesitamos m√°s overlap para preservar el contexto
    overlap_factor = 1.0 + (complexity * 0.6) 
    # Tambi√©n a√±adir factor por densidad sem√°ntica
    if analysis["semantic_density"] > 0.65:
        overlap_factor *= 1.15
    
    chunk_overlap = int(base_chunk_overlap * overlap_factor)
    
    # Restricciones para mantener valores razonables
    chunk_size = max(300, min(chunk_size, 1200))  # Entre 300 y 1200
    
    # El overlap debe ser proporcional al chunk_size pero no excesivo
    min_overlap = max(50, chunk_size // 8)  # Al menos 50 o 1/8 del chunk
    max_overlap = chunk_size // 3  # M√°ximo 1/3 del chunk para evitar duplicaci√≥n excesiva
    chunk_overlap = max(min_overlap, min(chunk_overlap, max_overlap))
    
    # Actualizar el an√°lisis con los valores recomendados
    analysis["recommended_chunk_size"] = chunk_size
    analysis["recommended_chunk_overlap"] = chunk_overlap



def get_smart_text_splitter(
    doc_extension: str, 
    config: IngestConfig, 
    # Par√°metros que ahora se pasan directamente
    chunk_size: int, 
    chunk_overlap: int,
    # doc_content es opcional y solo relevante si se quiere que esta funci√≥n a√∫n pueda determinar
    # los par√°metros si no se pasaron, pero la idea es que process_file ya lo haga.
    # Para este refactor, asumiremos que chunk_size y chunk_overlap ya vienen determinados.
):
    """
    Devuelve el splitter m√°s adecuado seg√∫n el tipo de documento.
    Chunk_size y chunk_overlap son determinados externamente y pasados como par√°metros.
    """
    logger.debug(f"Usando get_smart_text_splitter con chunk_size={chunk_size}, chunk_overlap={chunk_overlap} para extensi√≥n {doc_extension}")

    # Si no se usa chunking sem√°ntico, usar RecursiveCharacterTextSplitter simple
    if not config.semantic_chunking:
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""],
            length_function=len, # Es buena pr√°ctica definirla expl√≠citamente
        )
    
    # Personalizar el splitter seg√∫n el tipo de documento
    if doc_extension == "md":
        # Para archivos Markdown, usar splitter basado en encabezados
        headers_to_split_on = [
            ("#", "Heading1"),
            ("##", "Heading2"),
            ("###", "Heading3"),
            ("####", "Heading4"),
        ]
    
        # Para mantenerlo simple y alineado con la estructura de un solo splitter:
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[
                "\n\n",  # P√°rrafos
                "\n",    # L√≠neas
                "```",   # Bloques de c√≥digo
                "##",    # Encabezados
                "#",     # Encabezados
                ". ", 
                " ", 
                ""
            ],
            length_function=len,
        )
    
    elif doc_extension == "pdf" or doc_extension == "docx":
        # Para PDF y DOCX, usar separadores que respeten estructura de documento
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[
                "\n\n\n",  # Separaci√≥n entre secciones principales
                "\n\n",    # Separaci√≥n entre p√°rrafos
                "\n",      # Separaci√≥n entre l√≠neas
                ".\n",     # Puntos finales de p√°rrafo
                ". ",      # Separaci√≥n entre frases
                "; ",      # Separaci√≥n dentro de frases complejas
                ", ",      # Separaci√≥n dentro de frases
                " ",       # Separaci√≥n entre palabras
                ""         # √öltimo recurso: caracteres individuales
            ],
            length_function=len,
        )
    else: # txt y otros
        # Para otros tipos, usar configuraci√≥n est√°ndar
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""],
            length_function=len,
        )



def process_file(file_path: Path, config: IngestConfig, doc_processor: DocumentProcessor) -> List[Document]:
    """Procesa un archivo y devuelve los chunks resultantes"""
    try:
        logger.info(f"Procesando {file_path.name}...")
        
        cached_chunks = doc_processor.get_cached_chunks(file_path) # Movido al inicio para chequear cach√© primero
        
        if cached_chunks is not None and config.incremental: # Asegurarse que incremental est√© activo
            logger.info(f"Usando chunks cacheados para {file_path.name}")
            # Actualizar algunos metadatos que pueden cambiar si es necesario
            for chunk in cached_chunks:
                chunk.metadata["ingest_time"] = time.time() # Ejemplo de metadato a actualizar
            return cached_chunks

        # Si no hay cach√© o no es incremental, procesar
        loader = get_file_loader(file_path, config.file_extensions)
        docs = loader.load()
        
        if not docs:
            logger.warning(f"El loader no devolvi√≥ documentos para {file_path.name}")
            return []

        file_extension = file_path.suffix.lower().lstrip(".")
        
        full_content = "\n\n".join([doc.page_content for doc in docs if doc.page_content])
        
        if not full_content.strip():
            logger.warning(f"El contenido extra√≠do de {file_path.name} est√° vac√≠o.")
            return []

        current_chunk_size = config.chunk_size
        current_chunk_overlap = config.chunk_overlap
        doc_analysis_results = None # Para almacenar los resultados del an√°lisis

        if config.dynamic_chunking:
            doc_analysis_results = analyze_document_structure(full_content, file_extension)
            current_chunk_size = doc_analysis_results["recommended_chunk_size"]
            current_chunk_overlap = doc_analysis_results["recommended_chunk_overlap"]
            logger.info(
                f"Chunking din√°mico para {file_path.name}: "
                f"size={current_chunk_size}, overlap={current_chunk_overlap} "
                f"(doc tipo: {doc_analysis_results.get('doc_type', 'general')}, "
                f"densidad: {doc_analysis_results.get('semantic_density', 0):.2f})"
            )
        
        splitter = get_smart_text_splitter(
            file_extension, 
            config, 
            current_chunk_size, # Pasar tama√±o determinado
            current_chunk_overlap # Pasar overlap determinado
        )
        
        # Dividir en chunks
        # split_documents espera una lista de Documentos. Si docs ya es una lista de Documentos, est√° bien.
        chunks = splitter.split_documents(docs)
        
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            # Actualizar metadatos del chunk
            # Detectar t√≠tulo, secci√≥n o keywords (puede basarse en heur√≠sticas simples)
            first_line = chunk.page_content.strip().split('\n')[0]
            if len(first_line) < 80:
                chunk.metadata["title"] = first_line

            # Extraer secci√≥n por regex b√°sica (opcional)
            match_section = re.search(r"(?i)(secci√≥n|cap√≠tulo|t√≠tulo|parte)\s+\d+[:\-]?\s*(.+)", chunk.page_content[:300])
            if match_section:
                 chunk.metadata["section"] = match_section.group(2).strip()

            # Opcional: keywords (si ten√©s alg√∫n algoritmo o regex)
            keywords = re.findall(r'\b[A-Z]{3,}\b', chunk.page_content)
            if keywords:
                chunk.metadata["keywords"] = list(set(keywords))


            logger.info(f"üß™ DOCUMENT_ID={os.environ.get('DOCUMENT_ID')}")
            logger.info(f"üß™ file_path.parent.name={file_path.parent.name}")
            
            document_id = file_path.parent.name or file_path.stem
            chunk.metadata["document_id"] = document_id  # ‚Üê usa el nombre del directorio que es el documentId
            chunk.metadata["source"] = file_path.name
            chunk.metadata["extension"] = file_extension
            chunk.metadata["ingest_time"] = time.time()
            chunk.metadata["chunk_id"] = f"{file_path.name}-{i}" # ID de chunk m√°s √∫nico
            chunk.metadata["chunk_index"] = i
            chunk.metadata["chunk_total"] = len(chunks)
            chunk.metadata["file_size"] = file_path.stat().st_size
            # doc_type ya deber√≠a estar en doc_analysis_results si dynamic_chunking est√° activo
            chunk.metadata["doc_type"] = doc_analysis_results.get('doc_type', file_extension) if doc_analysis_results else file_extension
            chunk.metadata["process_date"] = datetime.now().isoformat()
            logger.info(f"üß™ document_id final seteado: {document_id}")


            if i > 0:
                chunk.metadata["prev_chunk_id"] = f"{file_path.name}-{i-1}"
            if i < len(chunks) - 1:
                chunk.metadata["next_chunk_id"] = f"{file_path.name}-{i+1}"
                
            if config.dynamic_chunking and doc_analysis_results:
                chunk.metadata["chunk_size_used"] = current_chunk_size
                chunk.metadata["chunk_overlap_used"] = current_chunk_overlap
                chunk.metadata["doc_semantic_density"] = doc_analysis_results.get("semantic_density")
                # Guardar una copia del an√°lisis sin los par√°metros ya usados para no ser redundante
                analysis_to_store = doc_analysis_results.copy()
                analysis_to_store.pop("recommended_chunk_size", None)
                analysis_to_store.pop("recommended_chunk_overlap", None)
                chunk.metadata["doc_analysis_details"] = analysis_to_store
            
            # Filtrar por idioma
            if config.allowed_languages:
                lang = detect_language(chunk.page_content) # Asumiendo que detect_language est√° definida
                if lang and lang in config.allowed_languages:
                    chunk.metadata["language"] = lang
                else:
                    logger.debug(f"Chunk de {file_path.name} (ID: {chunk.metadata['chunk_id']}) descartado por idioma: {lang if lang else 'no detectado'}")
                    continue # Saltar este chunk
            
            # A√±adir hash para deduplicaci√≥n
            chunk.metadata["content_hash"] = compute_text_hash(chunk.page_content) # Asumiendo que compute_text_hash est√° definida
            processed_chunks.append(chunk)
        
        if not processed_chunks:
            logger.info(f"No se generaron chunks v√°lidos (post-filtrado) para {file_path.name}")
            # A√∫n as√≠, marcar el archivo como procesado para no reintentar indefinidamente si el contenido es problem√°tico
            doc_processor.mark_file_processed(file_path)
            return []

        doc_processor.save_chunks_to_cache(file_path, processed_chunks)
        doc_processor.mark_file_processed(file_path)
        
        return processed_chunks
            
    except Exception as e:
        logger.error(f"Error procesando {file_path.name}: {e}", exc_info=True) # A√±adir exc_info para traceback
        return []


# Lock global para deduplicaci√≥n
dedup_lock = Lock()

def deduplicate_chunks(chunks: List[Document], similarity_threshold=0.95) -> List[Document]:
    """Elimina chunks duplicados o muy similares con sincronizaci√≥n y mejor detecci√≥n de similitudes"""
    with dedup_lock:
        seen_hashes = set()
        seen_ngrams = {}  # Para detectar similitud parcial
        unique_chunks = []
        
        for chunk in chunks:
            chunk_hash = chunk.metadata.get("content_hash", "")
            content = chunk.page_content.strip()
            
            # Si el hash ya existe, es un duplicado exacto
            if chunk_hash and chunk_hash in seen_hashes:
                continue
            
            # Verificar similitud por n-gramas para duplicados no exactos
            # Esto es m√°s r√°pido que comparar coseno de embeddings
            content_words = content.lower().split()
            if len(content_words) > 10:  # Solo para chunks de tama√±o razonable
                # Crear firma por 5-gramas
                content_ngrams = set()
                for i in range(len(content_words) - 5):
                    ngram = " ".join(content_words[i:i+5])
                    content_ngrams.add(ngram)
                
                # Verificar similitud con chunks previos
                is_similar = False
                for prev_id, prev_ngrams in seen_ngrams.items():
                    if len(content_ngrams) > 0 and len(prev_ngrams) > 0:
                        # Calcular coeficiente de Jaccard para medir similitud
                        overlap = len(content_ngrams.intersection(prev_ngrams))
                        similarity = overlap / len(content_ngrams.union(prev_ngrams))
                        
                        if similarity > similarity_threshold:
                            is_similar = True
                            break
                
                if is_similar:
                    continue
                
                # Guardar n-gramas para comparaciones futuras
                seen_ngrams[chunk.metadata.get("chunk_id", str(len(seen_ngrams)))] = content_ngrams
            
            # Marcamos como visto
            if chunk_hash:
                seen_hashes.add(chunk_hash)
                
            # A√±adimos a chunks √∫nicos
            unique_chunks.append(chunk)
    
    return unique_chunks


def batch_process_embeddings(chunks: List[Document], embeddings, batch_size: int) -> List[Tuple[Document, List[float]]]:
    """
    Procesa embeddings en batch para mejor rendimiento con manejo mejorado de errores y monitoreo
    Retorna lista de tuplas (documento, embedding)
    """
    if not chunks:
        return []
    
    # Dividir en lotes para embedding eficiente
    chunks_batches = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]
    results = []
    failed_chunks = []
    
    for i, batch in enumerate(tqdm(chunks_batches, desc="Procesando embeddings en batch", unit="batch")):
        # Extraer textos
        texts = [doc.page_content for doc in batch]
        
        try:
            # Calcular embeddings para todo el lote
            batch_embeddings = embeddings.embed_documents(texts)
            
            # Validar dimensiones
            if batch_embeddings and not all(len(emb) == len(batch_embeddings[0]) for emb in batch_embeddings):
                logger.warning(f"Batch {i+1}: Dimensiones inconsistentes en embeddings, intentando recuperar...")
                # Intentar procesar individualmente los documentos problem√°ticos
                for j, doc in enumerate(batch):
                    try:
                        single_emb = embeddings.embed_documents([doc.page_content])[0]
                        results.append((doc, single_emb))
                    except Exception as e_single:
                        logger.error(f"Error en embedding individual: {e_single}")
                        failed_chunks.append(doc)
                continue  # Pasar al siguiente batch
            
            # Asociar cada documento con su embedding
            for j, doc in enumerate(batch):
                if j < len(batch_embeddings):  # Verificar √≠ndices
                    results.append((doc, batch_embeddings[j]))
                else:
                    logger.error(f"√çndice fuera de rango: {j} >= {len(batch_embeddings)}")
                    failed_chunks.append(doc)
                    
        except Exception as e:
            logger.error(f"Error en batch {i+1}: {e}")
            # Intentar procesamiento uno por uno para recuperar los que se puedan
            for doc in batch:
                try:
                    single_emb = embeddings.embed_documents([doc.page_content])[0]
                    results.append((doc, single_emb))
                except Exception as e_single:
                    logger.error(f"Error en embedding individual: {e_single}")
                    failed_chunks.append(doc)
    
    # Informar sobre fallos
    if failed_chunks:
        logger.warning(f"No se pudieron procesar {len(failed_chunks)} chunks")
    
    return results


def parallel_embed_chunks(chunks: List[Document], embeddings, batch_size: int, workers: int) -> List[Tuple[Document, List[float]]]:
    """
    Calcula embeddings en paralelo usando m√∫ltiples workers con mejor manejo de errores
    y balanceo de carga din√°mico para mayor eficiencia.
    """
    if not chunks:
        return []
    
    # Si hay pocos chunks, procesarlos directamente sin paralelismo
    if len(chunks) <= batch_size or workers <= 1:
        return batch_process_embeddings(chunks, embeddings, batch_size)
    
    # Estrategia de distribuci√≥n mejorada: dividir en m√°s bloques que workers
    # para mejor balanceo din√°mico de carga
    chunks_per_worker = max(batch_size, len(chunks) // (workers * 3))
    chunk_blocks = [chunks[i:i + chunks_per_worker] for i in range(0, len(chunks), chunks_per_worker)]
    
    logger.info(f"Procesando embeddings en paralelo: {len(chunks)} chunks, {workers} workers, {len(chunk_blocks)} bloques")
    
    results = []
    failed_blocks = []
    
    # Usar ThreadPoolExecutor en lugar de ProcessPoolExecutor para compartir modelo
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {
            executor.submit(batch_process_embeddings, block, embeddings, batch_size): i
            for i, block in enumerate(chunk_blocks)
        }
        
        # Usar tqdm para monitoreo en tiempo real
        for future in tqdm(as_completed(futures), total=len(futures), desc="Calculando embeddings en paralelo"):
            block_idx = futures[future]
            try:
                batch_results = future.result()
                results.extend(batch_results)
                
                # Monitoreo peri√≥dico
                if len(results) % (batch_size * 5) == 0:
                    logger.info(f"Progreso: {len(results)}/{len(chunks)} chunks procesados ({len(results)/len(chunks)*100:.1f}%)")
                
            except Exception as e:
                logger.error(f"Error en bloque {block_idx}: {e}")
                failed_blocks.append(block_idx)
    
    # Intentar recuperar bloques fallidos con batch_size m√°s peque√±o
    if failed_blocks:
        logger.warning(f"Reintentando {len(failed_blocks)} bloques fallidos con batch_size reducido")
        for block_idx in failed_blocks:
            if block_idx < len(chunk_blocks):  # Verificar √≠ndice
                block = chunk_blocks[block_idx]
                try:
                    # Reintentar con batch_size m√°s peque√±o
                    recovery_results = batch_process_embeddings(block, embeddings, max(1, batch_size // 4))
                    results.extend(recovery_results)
                except Exception as e:
                    logger.error(f"Error en recuperaci√≥n de bloque {block_idx}: {e}")
    
    return results# En ingest.py

def insert_into_pgvector(
    doc_embed_pairs: List[Tuple[Document, List[float]]], 
    pg_conn: str, 
    collection_name: str, 
    embeddings_model_instance,
    config: IngestConfig
):
    """Inserta documentos y sus embeddings en la base de datos PGVector con mejor rendimiento y validaci√≥n"""
    if not doc_embed_pairs:
        logger.info("No hay documentos para insertar en PGVector.")
        return

    should_pre_delete = getattr(config, 'reset_vector_collection', False)

    try:
        logger.info(f"Conectando a PGVector. Colecci√≥n: {collection_name}. Reset colecci√≥n: {should_pre_delete}")
        
        # Validar conexi√≥n antes de proceder
        import psycopg2
        try:
            # Verificar conexi√≥n a la base de datos
            conn = psycopg2.connect(pg_conn)
            conn.close()
        except Exception as e:
            logger.error(f"Error al conectar con la base de datos: {e}")
            raise
            
        vector_store = PGVector(
            embedding_function=embeddings_model_instance,
            collection_name=collection_name,
            connection_string=pg_conn,
            pre_delete_collection=should_pre_delete 
        )

        total_inserted = 0
        db_batch_size = min(config.batch_size, 100)  # Limitar tama√±o de batch para evitar problemas de memoria
        num_batches = (len(doc_embed_pairs) + db_batch_size - 1) // db_batch_size

        logger.info(f"Iniciando inserci√≥n de {len(doc_embed_pairs)} embeddings en {num_batches} lotes de tama√±o {db_batch_size}.")

        # Usar tqdm con unidad espec√≠fica para mejor monitoreo
        for i in tqdm(range(num_batches), desc="Insertando en PGVector", unit="batch"):
            batch_pairs = doc_embed_pairs[i * db_batch_size : (i + 1) * db_batch_size]
            if not batch_pairs:
                continue

            texts_batch = [doc.page_content for doc, _ in batch_pairs]
            embeddings_batch = [emb for _, emb in batch_pairs]
            metadatas_batch = [doc.metadata for doc, _ in batch_pairs]
            
            # Validar que todos los embeddings tengan la misma dimensionalidad
            emb_lens = set(len(emb) for emb in embeddings_batch)
            if len(emb_lens) > 1:
                logger.warning(f"Dimensiones inconsistentes en embeddings: {emb_lens}")
                # Filtrar solo los que tienen la dimensi√≥n correcta (la m√°s com√∫n)
                from collections import Counter
                most_common_dim = Counter(len(emb) for emb in embeddings_batch).most_common(1)[0][0]
                
                valid_indices = [i for i, emb in enumerate(embeddings_batch) if len(emb) == most_common_dim]
                texts_batch = [texts_batch[i] for i in valid_indices]
                embeddings_batch = [embeddings_batch[i] for i in valid_indices]
                metadatas_batch = [metadatas_batch[i] for i in valid_indices]
                
                logger.info(f"Filtrados {len(batch_pairs) - len(valid_indices)} embeddings con dimensiones incorrectas")

            try:
                # Usar transacci√≥n √∫nica para el lote entero para mejor rendimiento
                vector_store.add_embeddings(
                    texts=texts_batch,
                    embeddings=embeddings_batch,
                    metadatas=metadatas_batch
                )
                total_inserted += len(texts_batch)
            except Exception as e_batch:
                logger.error(f"Error al insertar lote {i+1}/{num_batches}: {e_batch}")
                # Intentar insertar uno por uno para salvar los que se puedan
                for j in range(len(texts_batch)):
                    try:
                        vector_store.add_embeddings(
                            texts=[texts_batch[j]],
                            embeddings=[embeddings_batch[j]],
                            metadatas=[metadatas_batch[j]]
                        )
                        total_inserted += 1
                    except Exception:
                        # Simplemente seguimos adelante
                        pass
        
        logger.info(f"Insertados {total_inserted} documentos en la colecci√≥n '{collection_name}'.")
        
    except Exception as e_main:
        logger.error(f"Error general durante la inserci√≥n en PGVector: {e_main}", exc_info=True)
        raise

def improve_chunk_quality(chunks: List[Document]) -> List[Document]:
    """
    Mejora la calidad de los chunks para RAG mediante:
    - Enriquecimiento de metadatos
    - Limpieza y normalizaci√≥n
    - Filtrado de chunks de baja calidad
    - Mejor manejo de referencias cruzadas
    """
    if not chunks:
        return []
    
    improved_chunks = []
    
    # Agrupar chunks por fuente para facilitar referencias cruzadas
    chunks_by_source = {}
    for chunk in chunks:
        source = chunk.metadata.get("source", "unknown")
        if source not in chunks_by_source:
            chunks_by_source[source] = []
        chunks_by_source[source].append(chunk)
    
    for i, chunk in enumerate(chunks):
        # Filtrar chunks extremadamente cortos
        if len(chunk.page_content.strip()) < 50:
            continue
        
        # Normalizaci√≥n de contenido
        content = chunk.page_content
        
        # Eliminar m√∫ltiples saltos de l√≠nea y espacios extra
        content = re.sub(r'\n{3,}', '\n\n', content)
        content = re.sub(r' {2,}', ' ', content)
        content = content.strip()
        
        # Eliminar t√≠tulos duplicados al inicio del chunk
        lines = content.split('\n')
        if len(lines) > 2:
            if lines[0].strip() and (lines[0].strip() == lines[1].strip() or 
                                    lines[0].strip() in lines[1] or lines[1].strip() in lines[0]):
                content = '\n'.join(lines[1:])
        
        # An√°lisis de riqueza de contenido
        words = content.split()
        unique_words = set(w.lower() for w in words if len(w) > 3)
        
        # Filtrar chunks con muy poco contenido √∫nico
        if len(unique_words) < 10:
            continue
        
        # Obtener metadatos b√°sicos
        source = chunk.metadata.get("source", "unknown")
        chunk_index = chunk.metadata.get("chunk_index", i)
        chunk_total = chunk.metadata.get("chunk_total", len(chunks))
        
        # Extraer entidades potenciales (palabras que comienzan con may√∫scula despu√©s de puntuaci√≥n)
        entities = set()
        words_with_context = re.findall(r'[.!?]\s+([A-Z][a-zA-Z]*)', content)
        words_with_context.extend(re.findall(r'\b([A-Z][a-zA-Z]*\s+[A-Z][a-zA-Z]*)\b', content))
        if words_with_context:
            entities = set(words_with_context)
        
        # An√°lisis de frecuencia de palabras para keywords
        word_freq = {}
        for word in words:
            word = word.lower()
            if len(word) > 3 and word.isalnum():
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Stopwords en espa√±ol e ingl√©s
        stopwords = {'este', 'esta', 'estos', 'estas', 'aquellos', 'aquellas', 'para', 'como', 'desde', 'cuando', 'donde', 'porque', 'como', 'tambien', 'pero', 'the', 'and', 'that', 'this', 'with', 'from', 'have', 'for', 'not', 'was', 'were', 'are', 'they', 'their', 'them'}
        keywords = [w for w, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True) 
                  if w not in stopwords][:5]
        
        # Detectar secciones importantes basadas en patrones
        section_match = re.search(r"(?i)(secci√≥n|cap√≠tulo|t√≠tulo|parte)\s+[\dIVXLC]+[:\-]?\s*(.+?)(?=\n|$)", content[:300])
        section_name = section_match.group(2).strip() if section_match else None
        
        # Detectar fechas en el texto
        dates = re.findall(r'\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b|\b\d{1,2}\s+de\s+(?:enero|febrero|marzo|abril|mayo|junio|julio|agosto|septiembre|octubre|noviembre|diciembre)\s+(?:de\s+)?\d{2,4}\b', content)
        
        # Enriquecer metadatos
        chunk.metadata.update({
            "content_quality": len(unique_words) / max(1, len(words)),
            "processed_date": datetime.now().isoformat(),
            "word_count": len(words),
            "unique_word_count": len(unique_words),
            "entities": list(entities) if entities else None,
            "keywords": keywords,
            "position_percent": chunk_index / max(1, chunk_total - 1) if chunk_total > 1 else 0.5,
            "is_first": chunk_index == 0,
            "is_last": chunk_index == chunk_total - 1,
            "section": section_name,
            "dates": dates if dates else None
        })
        
        # Crear referencias a chunks vecinos para mejor navegaci√≥n y contexto
        source_chunks = chunks_by_source.get(source, [])
        if len(source_chunks) > 1:
            if chunk_index > 0:
                chunk.metadata["prev_chunk_ref"] = f"{source}:{chunk_index-1}"
            if chunk_index < chunk_total - 1:
                chunk.metadata["next_chunk_ref"] = f"{source}:{chunk_index+1}"
        
        # Analizar contexto sem√°ntico
        if len(content) > 100:
            first_paragraph = content.split('\n\n')[0] if '\n\n' in content else content[:min(300, len(content))]
            chunk.metadata["summary"] = first_paragraph.strip()
        
        chunk.page_content = content
        improved_chunks.append(chunk)
    
    return improved_chunks


def optimize_vector_search_config(pg_conn: str, collection_name: str):
    """
    Optimiza la configuraci√≥n de b√∫squeda vectorial en PGVector
    para mejorar el rendimiento y la relevancia.
    """
    try:
        import psycopg2
        conn = psycopg2.connect(pg_conn)
        cursor = conn.cursor()
        
        # Verificar existencia de √≠ndices
        safe_name = sanitize_pg_identifier(collection_name)
        cursor.execute(f"""
            SELECT indexname, indexdef
            FROM pg_indexes
            WHERE tablename = '{safe_name}'
        """)
        existing_indexes = cursor.fetchall()
        index_names = [idx[0] for idx in existing_indexes]
        
        # Crear √≠ndice GIN para b√∫squedas r√°pidas en metadatos
        metadata_index_name = f"{safe_name}_metadata_gin_idx"
        if metadata_index_name not in index_names:
            logger.info(f"Creando √≠ndice GIN para metadatos en {collection_name}")
            cursor.execute(f"""
                CREATE INDEX IF NOT EXISTS {metadata_index_name}
                ON public.{safe_name} USING GIN (metadata);
            """)
        
        # Optimizar √≠ndice vectorial existente para mejorar eficiencia
        cursor.execute(f"""
            ALTER INDEX public.{safe_name}_langchain_vector_idx
            SET (lists_growth_with_size = yes, intermediate_compression_threshold = 1024);
        """)
        
        # Crear vista materializada para consultas frecuentes
        view_name = f"{safe_name}_search_view"
        cursor.execute(f"""
            SELECT relname FROM pg_class 
            WHERE relkind = 'm' AND relname = '{view_name}'
        """)
        if not cursor.fetchone():
            logger.info(f"Creando vista materializada para b√∫squeda en {collection_name}")
            cursor.execute(f"""
                CREATE MATERIALIZED VIEW IF NOT EXISTS public.{view_name} AS
                SELECT uuid, document, embedding, metadata, 
                       metadata->>'source' as source,
                       metadata->>'language' as language,
                       metadata->>'doc_type' as doc_type,
                       metadata->>'chunk_id' as chunk_id
                FROM public.{safe_name};
                
                CREATE INDEX IF NOT EXISTS {view_name}_source_idx
                ON public.{view_name} (source);
                
                CREATE INDEX IF NOT EXISTS {view_name}_language_idx
                ON public.{view_name} (language);
                
                CREATE INDEX IF NOT EXISTS {view_name}_doc_type_idx
                ON public.{view_name} (doc_type);
            """)
        
        # Configurar par√°metros para optimizaci√≥n de b√∫squeda vectorial
        cursor.execute(f"""
            ALTER TABLE public.{safe_name}
            SET (autovacuum_vacuum_scale_factor = 0.05, autovacuum_analyze_scale_factor = 0.02);
        """)
        
        conn.commit()
        cursor.close()
        conn.close()
        logger.info(f"Optimizaci√≥n de b√∫squeda vectorial completada para {collection_name}")
        
    except Exception as e:
        logger.error(f"Error al optimizar b√∫squeda vectorial: {e}")


def monitor_vector_db_health(pg_conn: str, collection_name: str):
    """
    Monitora la salud de la base de datos vectorial y reporta estad√≠sticas.
    """
    try:
        import psycopg2
        conn = psycopg2.connect(pg_conn)
        cursor = conn.cursor()
        
        # Contar total de documentos
        cursor.execute(f"""
            SELECT COUNT(*) FROM public.{safe_name};
        """)
        total_docs = cursor.fetchone()[0]
        
        # Estad√≠sticas de metadatos
        cursor.execute(f"""
            SELECT 
                COUNT(DISTINCT metadata->>'source') as unique_sources,
                COUNT(DISTINCT metadata->>'language') as unique_languages,
                COUNT(DISTINCT metadata->>'doc_type') as unique_doc_types
            FROM public.{safe_name};
        """)
        metadata_stats = cursor.fetchone()
        
        # Estad√≠sticas de embeddings
        cursor.execute(f"""
            SELECT AVG(array_length(embedding, 1)) as avg_dimensions,
                   MIN(array_length(embedding, 1)) as min_dimensions,
                   MAX(array_length(embedding, 1)) as max_dimensions
            FROM public.{safe_name};
        """)
        dimension_stats = cursor.fetchone()
        
        # Estad√≠sticas de tama√±o y calidad
        cursor.execute(f"""
            SELECT 
                AVG((metadata->>'word_count')::float) as avg_words,
                AVG((metadata->>'content_quality')::float) as avg_quality,
                MIN((metadata->>'content_quality')::float) as min_quality,
                MAX((metadata->>'content_quality')::float) as max_quality
            FROM public.{safe_name}
            WHERE metadata->>'word_count' IS NOT NULL;
        """)
        content_stats = cursor.fetchone()
        
        # Informaci√≥n de fragmentaci√≥n
        cursor.execute(f"""
            SELECT 
                pg_size_pretty(pg_total_relation_size('public.{safe_name}')) as total_size,
                pg_size_pretty(pg_table_size('public.{safe_name}')) as table_size,
                pg_size_pretty(pg_indexes_size('public.{safe_name}')) as index_size
            FROM pg_catalog.pg_class c
            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
            WHERE c.relname = '{safe_name}' AND n.nspname = 'public';
        """)
        size_stats = cursor.fetchone()
        
        # Reportar estad√≠sticas
        logger.info(f"Estad√≠sticas de la base de datos vectorial ({collection_name}):")
        logger.info(f"  - Total de documentos: {total_docs}")
        logger.info(f"  - Fuentes √∫nicas: {metadata_stats[0]}")
        logger.info(f"  - Idiomas: {metadata_stats[1]}")
        logger.info(f"  - Tipos de documentos: {metadata_stats[2]}")
        logger.info(f"  - Dimensiones de embeddings: avg={dimension_stats[0]}, min={dimension_stats[1]}, max={dimension_stats[2]}")
        
        if content_stats[0]:
            logger.info(f"  - Promedio de palabras por chunk: {content_stats[0]:.2f}")
            logger.info(f"  - Calidad promedio de contenido: {content_stats[1]:.2f} (min={content_stats[2]:.2f}, max={content_stats[3]:.2f})")
        
        if size_stats:
            logger.info(f"  - Tama√±o total: {size_stats[0]}")
            logger.info(f"  - Tama√±o tabla: {size_stats[1]}")
            logger.info(f"  - Tama√±o √≠ndices: {size_stats[2]}")
        
        # Alertas
        if dimension_stats[1] != dimension_stats[2]:
            logger.warning(f"¬°ALERTA! Dimensiones inconsistentes en los embeddings: min={dimension_stats[1]}, max={dimension_stats[2]}")
        
        if content_stats and content_stats[2] < 0.2:
            logger.warning(f"¬°ALERTA! Chunks de baja calidad detectados (m√≠nimo: {content_stats[2]:.2f})")
        
        # Recomendaciones
        cursor.execute(f"""
            SELECT reltuples::bigint, relpages::bigint
            FROM pg_class
            WHERE relname = '{safe_name}';
        """)
        rel_stats = cursor.fetchone()
        
        if rel_stats and rel_stats[0] > 0 and rel_stats[0] / max(1, rel_stats[1]) < 10:
            logger.info("Recomendaci√≥n: Considerar realizar VACUUM FULL para optimizar espacio")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"Error al monitorear la base de datos vectorial: {e}")


def search_quality_validation(pg_conn: str, collection_name: str, embeddings, test_queries: List[str] = None):
    """
    Valida la calidad de b√∫squeda con consultas de prueba para ajustar par√°metros.
    """
    if not test_queries:
        test_queries = [
            "¬øCu√°les son los principales conceptos del documento?",
            "Resumen del contenido",
            "Aspectos m√°s importantes",
            "Informaci√≥n t√©cnica espec√≠fica",
            "Detalles legales relevantes"
        ]
    
    try:
        vector_store = PGVector(
            embedding_function=embeddings,
            collection_name=collection_name,
            connection_string=pg_conn
        )
        
        logger.info("Realizando validaci√≥n de calidad de b√∫squeda...")
        
        results = []
        for query in test_queries:
            logger.info(f"Consultando: '{query}'")
            
            # Probar diferentes configuraciones de b√∫squeda
            similarity_search_results = vector_store.similarity_search(query, k=3)
            mmr_results = vector_store.max_marginal_relevance_search(query, k=3, fetch_k=10)
            
            # Evaluar diversidad
            similarity_content = [doc.page_content[:100] for doc in similarity_search_results]
            mmr_content = [doc.page_content[:100] for doc in mmr_results]
            
            unique_sources_similarity = len(set([doc.metadata.get('source', '') for doc in similarity_search_results]))
            unique_sources_mmr = len(set([doc.metadata.get('source', '') for doc in mmr_results]))
            
            results.append({
                'query': query,
                'similarity_sources': unique_sources_similarity,
                'mmr_sources': unique_sources_mmr,
                'similarity_vs_mmr_overlap': len(set(similarity_content) & set(mmr_content))
            })
        
        # Analizar resultados
        avg_similarity_sources = sum(r['similarity_sources'] for r in results) / len(results)
        avg_mmr_sources = sum(r['mmr_sources'] for r in results) / len(results)
        avg_overlap = sum(r['similarity_vs_mmr_overlap'] for r in results) / len(results)
        
        logger.info(f"Resultados de validaci√≥n:")
        logger.info(f"  - Promedio de fuentes por consulta (similarity): {avg_similarity_sources:.2f}")
        logger.info(f"  - Promedio de fuentes por consulta (MMR): {avg_mmr_sources:.2f}")
        logger.info(f"  - Superposici√≥n media entre m√©todos: {avg_overlap:.2f}")
        
        # Recomendaciones basadas en resultados
        if avg_overlap < 1.5:
            logger.info("Recomendaci√≥n: La diversidad entre m√©todos es alta. MMR proporciona resultados significativamente diferentes.")
        else:
            logger.info("Recomendaci√≥n: Considerar aumentar la diversidad con MMR ajustando el par√°metro lambda.")
        
        if avg_mmr_sources > avg_similarity_sources * 1.2:
            logger.info("Recomendaci√≥n: MMR muestra mayor diversidad de fuentes. Recomendado para consultas exploratorias.")
        else:
            logger.info("Recomendaci√≥n: La b√∫squeda por similitud y MMR muestran diversidad comparable de fuentes.")
        
        return results
        
    except Exception as e:
        logger.error(f"Error en validaci√≥n de calidad: {e}")
        return []


def get_hybrid_retriever(pg_conn: str, collection_name: str, embeddings, use_compression: bool = False):
    """
    Crea un retriever h√≠brido que combina b√∫squeda vectorial y por keywords
    para mejorar la calidad y relevancia de resultados.
    """
    try:
        # Crear almac√©n vectorial
        vector_store = PGVector(
            embedding_function=embeddings,
            collection_name=collection_name,
            connection_string=pg_conn
        )
        
        # Vector retriever base
        vector_retriever = vector_store.as_retriever(
            search_type="mmr",  # Maximum Marginal Relevance
            search_kwargs={"k": 5, "fetch_k": 20, "lambda_mult": 0.7}
        )
        
        # Si se requiere compresi√≥n para chunks extensos
        if use_compression:
            from langchain.retrievers import ContextualCompressionRetriever
            from langchain.retrievers.document_compressors import LLMChainExtractor
            
            # Nota: Esto requiere configurar un LLM para la compresi√≥n
            # llm = ... (configurar OpenAI, Anthropic, etc.)
            # compressor = LLMChainExtractor.from_llm(llm)
            # compressed_retriever = ContextualCompressionRetriever(
            #     base_compressor=compressor,
            #     base_retriever=vector_retriever
            # )
            # return compressed_retriever
        
        # Crear un retriever de b√∫squeda por keywords usando metadatos
        # Esto es una simplificaci√≥n, para implementaci√≥n real se necesitar√≠a desarrollar esto
        
        # Combinar ambos retrievers en una funci√≥n
        def hybrid_retrieve(query: str, **kwargs):
            k = kwargs.get("k", 5)
            
            # Vector search
            vector_docs = vector_retriever.get_relevant_documents(query)
            
            # Extraer keywords usando alguna biblioteca como KeyBERT o m√©todos simples
            keywords = set(re.findall(r'\b\w{4,}\b', query.lower()))
            
            # Aumentar resultados con una re-clasificaci√≥n basada en coincidencia de keywords
            for doc in vector_docs:
                doc_keywords = set(re.findall(r'\b\w{4,}\b', doc.page_content.lower()))
                keyword_match_score = len(keywords.intersection(doc_keywords)) / max(1, len(keywords))
                doc.metadata["keyword_score"] = keyword_match_score
            
            # Ordenar por score combinado (ya ordenados por vector + boost por keywords)
            reranked_docs = sorted(
                vector_docs, 
                key=lambda d: d.metadata.get("keyword_score", 0), 
                reverse=True
            )
            
            return reranked_docs[:k]
        
        # Crear un objeto RetrieverLike que encapsule nuestra funci√≥n h√≠brida
        class HybridRetriever:
            def __init__(self, retriever_func):
                self.retrieve = retriever_func
                
            def get_relevant_documents(self, query: str, **kwargs):
                return self.retrieve(query, **kwargs)
                
        return HybridRetriever(hybrid_retrieve)
        
    except Exception as e:
        logger.error(f"Error al crear retriever h√≠brido: {e}")
        # Fallback al retriever b√°sico
        return vector_store.as_retriever()


def main():
    """Funci√≥n principal del proceso de ingesta optimizada para rendimiento y calidad"""
    try:
        # Cargar configuraci√≥n
        config, pg_conn = load_config()
        logger.info(f"ARGUMENTOS RECIBIDOS:" ,sys.argv)
        print("üß™ ARGUMENTOS RECIBIDOS:", sys.argv)
        logger.info(f"Configuraci√≥n de ingesta cargada: {vars(config)}")

        # Verificar directorio de documentos
        if not config.docs_dir.exists():
            raise FileNotFoundError(f"El directorio de documentos '{config.docs_dir}' no existe")
        if not config.docs_dir.is_dir():
            raise NotADirectoryError(f"La ruta de documentos '{config.docs_dir}' no es un directorio")

        # Iniciar medici√≥n de rendimiento
        overall_start_time = time.time()
        stage_times = {}
        memory_usage = []

        # Monitorear memoria inicial
        memory_usage.append(psutil.Process().memory_info().rss / 1024**2)
        logger.info(f"Memoria inicial: {memory_usage[-1]:.2f} MB")

        try:
            # 1. Cargar modelo de embeddings
            model_load_start = time.time()
            logger.info(f"Cargando modelo de embeddings: {config.embed_model}...")
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model_kwargs = {
                "device": device
            }
            encode_kwargs = {
                "normalize_embeddings": True,
                "batch_size": config.batch_size
            }
            
            embeddings = HuggingFaceEmbeddings(
                model_name=config.embed_model,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs
            )
            
            stage_times["load_model"] = time.time() - model_load_start
            logger.info(f"Modelo de embeddings cargado en {stage_times['load_model']:.2f}s")
            logger.info(f"Dimensi√≥n de embeddings: {embeddings.client.get_sentence_embedding_dimension()}")
            
            # Monitorear memoria despu√©s de cargar modelo
            memory_usage.append(psutil.Process().memory_info().rss / 1024**2)
            logger.info(f"Memoria despu√©s de cargar modelo: {memory_usage[-1]:.2f} MB")

            # 2. Inicializar DocumentProcessor y obtener archivos
            doc_processor = DocumentProcessor(config)
            file_scan_start = time.time()
            files_to_process_info = doc_processor.get_files_to_process()
            stage_times["file_scan"] = time.time() - file_scan_start

            total_files_found = len(files_to_process_info)
            files_needing_processing = [(fp, needs_proc) for fp, needs_proc in files_to_process_info if needs_proc]
            num_files_to_process = len(files_needing_processing)
            
            total_size_mb = sum(f.stat().st_size for f, _ in files_to_process_info) / (1024 * 1024)
            processing_size_mb = sum(f.stat().st_size for f, _ in files_needing_processing) / (1024 * 1024)

            logger.info(f"Escaneo de archivos completado en {stage_times['file_scan']:.2f}s")
            logger.info(f"Archivos encontrados: {total_files_found} (Total: {total_size_mb:.2f} MB)")
            logger.info(f"Archivos a procesar: {num_files_to_process} (Tama√±o: {processing_size_mb:.2f} MB)")

            if num_files_to_process == 0:
                logger.info("No hay archivos nuevos para procesar")
                return

            # 3. Procesar archivos en paralelo
            processing_start = time.time()
            all_chunks = []
            
            with ProcessPoolExecutor(max_workers=config.max_workers) as executor:
                futures = []
                for file_path, needs_processing in files_needing_processing:
                    if needs_processing:
                        futures.append(
                            executor.submit(process_file, file_path, config, doc_processor)
                        )
                
                # Procesar resultados con barra de progreso
                for future in tqdm(as_completed(futures), total=len(futures), desc="Procesando archivos"):
                    try:
                        chunks = future.result()
                        if chunks:
                            all_chunks.extend(chunks)
                    except Exception as e:
                        logger.error(f"Error procesando archivo: {e}")
                        continue

            stage_times["file_processing"] = time.time() - processing_start
            logger.info(f"Procesamiento de archivos completado en {stage_times['file_processing']:.2f}s")
            logger.info(f"Chunks generados: {len(all_chunks)}")

            # Monitorear memoria despu√©s de procesar archivos
            memory_usage.append(psutil.Process().memory_info().rss / 1024**2)
            logger.info(f"Memoria despu√©s de procesar archivos: {memory_usage[-1]:.2f} MB")

            if not all_chunks:
                logger.warning("No se generaron chunks. Verificar archivos y configuraci√≥n.")
                return

            # 4. Mejorar calidad de chunks
            quality_start = time.time()
            improved_chunks = improve_chunk_quality(all_chunks)
            stage_times["quality_improvement"] = time.time() - quality_start
            logger.info(f"Mejora de calidad completada en {stage_times['quality_improvement']:.2f}s")
            logger.info(f"Chunks despu√©s de mejora: {len(improved_chunks)}")

            # 5. Deduplicar chunks si est√° habilitado
            if config.deduplication:
                dedup_start = time.time()
                final_chunks = deduplicate_chunks(improved_chunks, config.similarity_threshold)
                stage_times["deduplication"] = time.time() - dedup_start
                logger.info(f"Deduplicaci√≥n completada en {stage_times['deduplication']:.2f}s")
                logger.info(f"Chunks despu√©s de deduplicaci√≥n: {len(final_chunks)}")
            else:
                final_chunks = improved_chunks

            # 6. Generar embeddings
            embedding_start_time = time.time()
            
            # Usar parallel_embed_chunks o batch_process_embeddings seg√∫n configuraci√≥n
            if len(final_chunks) > encode_kwargs["batch_size"] * 2 and config.max_embed_workers > 1:
                doc_embed_pairs = parallel_embed_chunks(
                    final_chunks,
                    embeddings,
                    encode_kwargs["batch_size"],
                    config.max_embed_workers
                )
            else:
                logger.info("Procesando embeddings en un solo hilo")
                doc_embed_pairs = batch_process_embeddings(
                    final_chunks,
                    embeddings,
                    encode_kwargs["batch_size"]
                )
            
            stage_times["embedding_generation"] = time.time() - embedding_start_time
            logger.info(f"Generaci√≥n de embeddings completada en {stage_times['embedding_generation']:.2f}s")
            logger.info(f"Pares Doc-Embedding generados: {len(doc_embed_pairs)}")

            # Monitorear memoria despu√©s de generar embeddings
            memory_usage.append(psutil.Process().memory_info().rss / 1024**2)
            logger.info(f"Memoria despu√©s de generar embeddings: {memory_usage[-1]:.2f} MB")

            if not doc_embed_pairs:
                logger.error("No se generaron embeddings. Verificar modelo y datos.")
                return

            # 7. Inserci√≥n en PGVector
            db_insert_start_time = time.time()
            logger.info(f"Insertando {len(doc_embed_pairs)} embeddings en PGVector...")
            print(f"üß™ INSERTANDO en colecci√≥n: {config.collection_name}")
            print(f"üß™ RESET COLLECTION: {config.reset_vector_collection}")
            insert_into_pgvector(doc_embed_pairs, pg_conn, config.collection_name, embeddings, config)
            stage_times["db_insertion"] = time.time() - db_insert_start_time
            logger.info(f"Inserci√≥n en base de datos completada en {stage_times['db_insertion']:.2f}s")

            # 8. Optimizaci√≥n y validaci√≥n
            optimization_start = time.time()
            
            # Optimizar configuraci√≥n de b√∫squeda
            optimize_vector_search_config(pg_conn, config.collection_name)
            
            # Validar calidad de b√∫squeda
            search_quality_validation(pg_conn, config.collection_name, embeddings)
            
            stage_times["optimization"] = time.time() - optimization_start
            logger.info(f"Optimizaci√≥n y validaci√≥n completadas en {stage_times['optimization']:.2f}s")

            # 9. Monitoreo final
            total_time = time.time() - overall_start_time
            memory_usage.append(psutil.Process().memory_info().rss / 1024**2)
            
            # Generar reporte de rendimiento
            performance_report = {
                "total_time": total_time,
                "stage_times": stage_times,
                "memory_usage": {
                    "initial": memory_usage[0],
                    "after_model": memory_usage[1],
                    "after_processing": memory_usage[2],
                    "after_embeddings": memory_usage[3],
                    "final": memory_usage[4]
                },
                "files_processed": num_files_to_process,
                "total_chunks": len(all_chunks),
                "final_chunks": len(final_chunks),
                "embeddings_generated": len(doc_embed_pairs)
            }
            
            logger.info("Reporte de rendimiento:")
            logger.info(json.dumps(performance_report, indent=2))

            # 10. Limpieza
            if config.reset_vector_collection:
                logger.info("Limpieza de recursos...")
                # Aqu√≠ se podr√≠an agregar tareas de limpieza adicionales

        except Exception as e:
            logger.error(f"Error durante el procesamiento: {e}")
            logger.error(traceback.format_exc())
            raise

    except Exception as e:
        logger.critical(f"Error cr√≠tico en el proceso de ingesta: {e}")
        logger.critical(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()
